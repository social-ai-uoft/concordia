{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Case scenario: Testing MCAT Questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "path = os.path.abspath('..')\n",
    "if path not in sys.path:\n",
    "    sys.path.insert(0, path)\n",
    "\n",
    "import collections\n",
    "import concurrent.futures\n",
    "import datetime\n",
    "\n",
    "# from google.colab import widgets\n",
    "from IPython import display\n",
    "\n",
    "from concordia import components as generic_components\n",
    "from concordia.agents import basic_agent\n",
    "from concordia.components import agent as components\n",
    "from concordia.agents import basic_agent\n",
    "from concordia.associative_memory import associative_memory\n",
    "from concordia.associative_memory import blank_memories\n",
    "from concordia.associative_memory import formative_memories\n",
    "from concordia.associative_memory import importance_function\n",
    "from concordia.clocks import game_clock\n",
    "from concordia.components import game_master as gm_components\n",
    "from concordia.environment import game_master\n",
    "from concordia.metrics import goal_achievement\n",
    "from concordia.metrics import common_sense_morality\n",
    "from concordia.metrics import opinion_of_others\n",
    "from concordia.utils import measurements as measurements_lib\n",
    "from concordia.utils import html as html_lib\n",
    "from concordia.utils import plotting\n",
    "\n",
    "import logging\n",
    "logging.basicConfig(level=logging.ERROR, filename='components_testing.log')\n",
    "logger = logging.getLogger('ollama')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/socialai/.pyenv/versions/3.11.1/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Setup sentence encoder\n",
    "from sentence_transformers import SentenceTransformer\n",
    "st5_model = SentenceTransformer('sentence-transformers/sentence-t5-base')\n",
    "embedder = st5_model.encode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from concordia.language_model import ollama_model\n",
    "model = ollama_model.OllamaLanguageModel(\n",
    "    # model_name='llama2:70b',\n",
    "    model_name='mixtral'\n",
    "    # streaming=True\n",
    ")\n",
    "\n",
    "# import dotenv\n",
    "# import os\n",
    "# dotenv.load_dotenv()\n",
    "# api_key = os.getenv('OPENAI_API_KEY')\n",
    "\n",
    "# from concordia.language_model import gpt_model\n",
    "# model = gpt_model.GptLanguageModel(\n",
    "#     api_key=api_key,\n",
    "#     model_name='gpt-4',\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Make the clock\n",
    "time_step = datetime.timedelta(minutes=20)\n",
    "SETUP_TIME = datetime.datetime(hour=20, year=2024, month=10, day=1)\n",
    "\n",
    "START_TIME = datetime.datetime(hour=18, year=2024, month=10, day=2)\n",
    "clock = game_clock.MultiIntervalClock(\n",
    "    start=SETUP_TIME,\n",
    "    step_sizes=[time_step, datetime.timedelta(seconds=10)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "measurements = measurements_lib.Measurements()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/socialai/.pyenv/versions/3.11.1/lib/python3.11/site-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The function `__call__` was deprecated in LangChain 0.1.7 and will be removed in 0.2.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "ename": "OllamaEndpointNotFoundError",
     "evalue": "Ollama call failed with status code 404. Maybe your model is not found and you should pull the model with `ollama pull llama2`.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOllamaEndpointNotFoundError\u001b[0m               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 45\u001b[0m\n\u001b[1;32m     20\u001b[0m context \u001b[39m=\u001b[39m \u001b[39m\"\"\"\u001b[39m\u001b[39mElizabeth Harding, born in 1500 in the rural countryside of England, \u001b[39m\n\u001b[1;32m     21\u001b[0m \u001b[39mis a figure deeply entrenched in the tapestries of early Tudor England. Growing up in a \u001b[39m\n\u001b[1;32m     22\u001b[0m \u001b[39mmodest farming family, Elizabeth\u001b[39m\u001b[39m'\u001b[39m\u001b[39ms life has been shaped by the agrarian rhythms of sowing and \u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     41\u001b[0m \n\u001b[1;32m     42\u001b[0m \u001b[39m\"\"\"\u001b[39m\n\u001b[1;32m     44\u001b[0m quiz_metric \u001b[39m=\u001b[39m qm\u001b[39m.\u001b[39mQuizMetric(model, \u001b[39m\"\u001b[39m\u001b[39mElizabeth\u001b[39m\u001b[39m\"\u001b[39m, clock, \u001b[39m\"\u001b[39m\u001b[39m./custom_components/trivia_questions.json\u001b[39m\u001b[39m\"\u001b[39m, measurements\u001b[39m=\u001b[39mmeasurements, verbose\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m---> 45\u001b[0m quiz_metric\u001b[39m.\u001b[39;49mobserve(context)\n",
      "File \u001b[0;32m~/Downloads/concordia-component-testing/examples/custom_components/quiz_metric.py:81\u001b[0m, in \u001b[0;36mQuizMetric.observe\u001b[0;34m(self, observation)\u001b[0m\n\u001b[1;32m     79\u001b[0m question \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_exam[\u001b[39m\"\u001b[39m\u001b[39mquestions\u001b[39m\u001b[39m\"\u001b[39m][i]\n\u001b[1;32m     80\u001b[0m doc \u001b[39m=\u001b[39m interactive_document\u001b[39m.\u001b[39mInteractiveDocument(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_model)\n\u001b[0;32m---> 81\u001b[0m agent_answer \u001b[39m=\u001b[39m doc\u001b[39m.\u001b[39;49mmultiple_choice_question(\n\u001b[1;32m     82\u001b[0m     \u001b[39mf\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m{\u001b[39;49;00mobservation\u001b[39m}\u001b[39;49;00m\u001b[39m\\n\u001b[39;49;00m\u001b[39m{\u001b[39;49;00mquestion[\u001b[39m'\u001b[39;49m\u001b[39mquestion\u001b[39;49m\u001b[39m'\u001b[39;49m]\u001b[39m}\u001b[39;49;00m\u001b[39m\"\u001b[39;49m, question[\u001b[39m\"\u001b[39;49m\u001b[39moptions\u001b[39;49m\u001b[39m\"\u001b[39;49m]\n\u001b[1;32m     83\u001b[0m )\n\u001b[1;32m     85\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_verbose:\n\u001b[1;32m     86\u001b[0m     \u001b[39mprint\u001b[39m(\n\u001b[1;32m     87\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mQuestion: \u001b[39m\u001b[39m{\u001b[39;00mquestion[\u001b[39m'\u001b[39m\u001b[39mquestion\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_player_name\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39ms answer: \u001b[39m\u001b[39m{\u001b[39;00mquestion[\u001b[39m'\u001b[39m\u001b[39moptions\u001b[39m\u001b[39m'\u001b[39m][agent_answer]\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39mCorrect answer: \u001b[39m\u001b[39m{\u001b[39;00mquestion[\u001b[39m'\u001b[39m\u001b[39moptions\u001b[39m\u001b[39m'\u001b[39m][question[\u001b[39m'\u001b[39m\u001b[39mcorrect_answer\u001b[39m\u001b[39m'\u001b[39m]]\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m     88\u001b[0m     )\n",
      "File \u001b[0;32m~/Downloads/concordia-component-testing/concordia/document/interactive_document.py:196\u001b[0m, in \u001b[0;36mInteractiveDocument.multiple_choice_question\u001b[0;34m(self, question, answers)\u001b[0m\n\u001b[1;32m    193\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_question(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m  (\u001b[39m\u001b[39m{\u001b[39;00mkey\u001b[39m}\u001b[39;00m\u001b[39m) \u001b[39m\u001b[39m{\u001b[39;00moption\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[1;32m    195\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_response(\u001b[39m'\u001b[39m\u001b[39mAnswer: (\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m--> 196\u001b[0m idx, response, debug \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_model\u001b[39m.\u001b[39;49msample_choice(\n\u001b[1;32m    197\u001b[0m     prompt\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_model_view\u001b[39m.\u001b[39;49mtext(),\n\u001b[1;32m    198\u001b[0m     responses\u001b[39m=\u001b[39;49m\u001b[39mlist\u001b[39;49m(options\u001b[39m.\u001b[39;49mkeys()),\n\u001b[1;32m    199\u001b[0m )\n\u001b[1;32m    200\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_model_response(response)\n\u001b[1;32m    201\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_response(\u001b[39m'\u001b[39m\u001b[39m)\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m'\u001b[39m)\n",
      "File \u001b[0;32m~/Downloads/concordia-component-testing/concordia/language_model/ollama_model.py:124\u001b[0m, in \u001b[0;36mOllamaLanguageModel.sample_choice\u001b[0;34m(self, prompt, responses, seed)\u001b[0m\n\u001b[1;32m    122\u001b[0m attempts \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    123\u001b[0m \u001b[39mfor\u001b[39;00m _ \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(_MAX_MULTIPLE_CHOICE_ATTEMPTS):\n\u001b[0;32m--> 124\u001b[0m     sample \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msample_text(\n\u001b[1;32m    125\u001b[0m         prompt,\n\u001b[1;32m    126\u001b[0m         max_characters\u001b[39m=\u001b[39;49mmax_characters,\n\u001b[1;32m    127\u001b[0m         temperature\u001b[39m=\u001b[39;49m\u001b[39m0.0\u001b[39;49m,\n\u001b[1;32m    128\u001b[0m         seed\u001b[39m=\u001b[39;49mseed,\n\u001b[1;32m    129\u001b[0m         system_message\u001b[39m=\u001b[39;49mMULTIPLE_CHOICE_SYSTEM_MESSAGE,\n\u001b[1;32m    130\u001b[0m     )\n\u001b[1;32m    131\u001b[0m     answer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mextract_choices(sample)\n\u001b[1;32m    132\u001b[0m     \u001b[39m# Sarah Xu: Added print statements on 01/18/23 to view raw response\u001b[39;00m\n",
      "File \u001b[0;32m~/Downloads/concordia-component-testing/concordia/language_model/ollama_model.py:82\u001b[0m, in \u001b[0;36mOllamaLanguageModel.sample_text\u001b[0;34m(self, prompt, max_tokens, max_characters, terminators, temperature, timeout, seed, system_message)\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[39mfor\u001b[39;00m retry \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(_MAX_SAMPLE_TEXT_ATTEMPTS): \n\u001b[1;32m     81\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 82\u001b[0m         response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_client(\n\u001b[1;32m     83\u001b[0m             prompt,\n\u001b[1;32m     84\u001b[0m             stop\u001b[39m=\u001b[39;49mterminators,\n\u001b[1;32m     85\u001b[0m             temperature\u001b[39m=\u001b[39;49mtemperature,\n\u001b[1;32m     86\u001b[0m         )\n\u001b[1;32m     87\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mValueError\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     88\u001b[0m         logger\u001b[39m.\u001b[39merror(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mError while calling LLM with input: \u001b[39m\u001b[39m{\u001b[39;00mprompt\u001b[39m}\u001b[39;00m\u001b[39m. Attempt \u001b[39m\u001b[39m{\u001b[39;00mretry\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m failed.\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.1/lib/python3.11/site-packages/langchain_core/_api/deprecation.py:145\u001b[0m, in \u001b[0;36mdeprecated.<locals>.deprecate.<locals>.warning_emitting_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    143\u001b[0m     warned \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m    144\u001b[0m     emit_warning()\n\u001b[0;32m--> 145\u001b[0m \u001b[39mreturn\u001b[39;00m wrapped(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.1/lib/python3.11/site-packages/langchain_core/language_models/llms.py:948\u001b[0m, in \u001b[0;36mBaseLLM.__call__\u001b[0;34m(self, prompt, stop, callbacks, tags, metadata, **kwargs)\u001b[0m\n\u001b[1;32m    941\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(prompt, \u001b[39mstr\u001b[39m):\n\u001b[1;32m    942\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    943\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mArgument `prompt` is expected to be a string. Instead found \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    944\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtype\u001b[39m(prompt)\u001b[39m}\u001b[39;00m\u001b[39m. If you want to run the LLM on multiple prompts, use \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    945\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m`generate` instead.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    946\u001b[0m     )\n\u001b[1;32m    947\u001b[0m \u001b[39mreturn\u001b[39;00m (\n\u001b[0;32m--> 948\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgenerate(\n\u001b[1;32m    949\u001b[0m         [prompt],\n\u001b[1;32m    950\u001b[0m         stop\u001b[39m=\u001b[39;49mstop,\n\u001b[1;32m    951\u001b[0m         callbacks\u001b[39m=\u001b[39;49mcallbacks,\n\u001b[1;32m    952\u001b[0m         tags\u001b[39m=\u001b[39;49mtags,\n\u001b[1;32m    953\u001b[0m         metadata\u001b[39m=\u001b[39;49mmetadata,\n\u001b[1;32m    954\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m    955\u001b[0m     )\n\u001b[1;32m    956\u001b[0m     \u001b[39m.\u001b[39mgenerations[\u001b[39m0\u001b[39m][\u001b[39m0\u001b[39m]\n\u001b[1;32m    957\u001b[0m     \u001b[39m.\u001b[39mtext\n\u001b[1;32m    958\u001b[0m )\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.1/lib/python3.11/site-packages/langchain_core/language_models/llms.py:698\u001b[0m, in \u001b[0;36mBaseLLM.generate\u001b[0;34m(self, prompts, stop, callbacks, tags, metadata, run_name, **kwargs)\u001b[0m\n\u001b[1;32m    682\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    683\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mAsked to cache, but no cache found at `langchain.cache`.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    684\u001b[0m         )\n\u001b[1;32m    685\u001b[0m     run_managers \u001b[39m=\u001b[39m [\n\u001b[1;32m    686\u001b[0m         callback_manager\u001b[39m.\u001b[39mon_llm_start(\n\u001b[1;32m    687\u001b[0m             dumpd(\u001b[39mself\u001b[39m),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    696\u001b[0m         )\n\u001b[1;32m    697\u001b[0m     ]\n\u001b[0;32m--> 698\u001b[0m     output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_generate_helper(\n\u001b[1;32m    699\u001b[0m         prompts, stop, run_managers, \u001b[39mbool\u001b[39;49m(new_arg_supported), \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs\n\u001b[1;32m    700\u001b[0m     )\n\u001b[1;32m    701\u001b[0m     \u001b[39mreturn\u001b[39;00m output\n\u001b[1;32m    702\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(missing_prompts) \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.1/lib/python3.11/site-packages/langchain_core/language_models/llms.py:562\u001b[0m, in \u001b[0;36mBaseLLM._generate_helper\u001b[0;34m(self, prompts, stop, run_managers, new_arg_supported, **kwargs)\u001b[0m\n\u001b[1;32m    560\u001b[0m     \u001b[39mfor\u001b[39;00m run_manager \u001b[39min\u001b[39;00m run_managers:\n\u001b[1;32m    561\u001b[0m         run_manager\u001b[39m.\u001b[39mon_llm_error(e, response\u001b[39m=\u001b[39mLLMResult(generations\u001b[39m=\u001b[39m[]))\n\u001b[0;32m--> 562\u001b[0m     \u001b[39mraise\u001b[39;00m e\n\u001b[1;32m    563\u001b[0m flattened_outputs \u001b[39m=\u001b[39m output\u001b[39m.\u001b[39mflatten()\n\u001b[1;32m    564\u001b[0m \u001b[39mfor\u001b[39;00m manager, flattened_output \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(run_managers, flattened_outputs):\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.1/lib/python3.11/site-packages/langchain_core/language_models/llms.py:549\u001b[0m, in \u001b[0;36mBaseLLM._generate_helper\u001b[0;34m(self, prompts, stop, run_managers, new_arg_supported, **kwargs)\u001b[0m\n\u001b[1;32m    539\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_generate_helper\u001b[39m(\n\u001b[1;32m    540\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    541\u001b[0m     prompts: List[\u001b[39mstr\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    545\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any,\n\u001b[1;32m    546\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m LLMResult:\n\u001b[1;32m    547\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    548\u001b[0m         output \u001b[39m=\u001b[39m (\n\u001b[0;32m--> 549\u001b[0m             \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_generate(\n\u001b[1;32m    550\u001b[0m                 prompts,\n\u001b[1;32m    551\u001b[0m                 stop\u001b[39m=\u001b[39;49mstop,\n\u001b[1;32m    552\u001b[0m                 \u001b[39m# TODO: support multiple run managers\u001b[39;49;00m\n\u001b[1;32m    553\u001b[0m                 run_manager\u001b[39m=\u001b[39;49mrun_managers[\u001b[39m0\u001b[39;49m] \u001b[39mif\u001b[39;49;00m run_managers \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m    554\u001b[0m                 \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m    555\u001b[0m             )\n\u001b[1;32m    556\u001b[0m             \u001b[39mif\u001b[39;00m new_arg_supported\n\u001b[1;32m    557\u001b[0m             \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_generate(prompts, stop\u001b[39m=\u001b[39mstop)\n\u001b[1;32m    558\u001b[0m         )\n\u001b[1;32m    559\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mBaseException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    560\u001b[0m         \u001b[39mfor\u001b[39;00m run_manager \u001b[39min\u001b[39;00m run_managers:\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.1/lib/python3.11/site-packages/langchain_community/llms/ollama.py:400\u001b[0m, in \u001b[0;36mOllama._generate\u001b[0;34m(self, prompts, stop, images, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    398\u001b[0m generations \u001b[39m=\u001b[39m []\n\u001b[1;32m    399\u001b[0m \u001b[39mfor\u001b[39;00m prompt \u001b[39min\u001b[39;00m prompts:\n\u001b[0;32m--> 400\u001b[0m     final_chunk \u001b[39m=\u001b[39m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m_stream_with_aggregation(\n\u001b[1;32m    401\u001b[0m         prompt,\n\u001b[1;32m    402\u001b[0m         stop\u001b[39m=\u001b[39;49mstop,\n\u001b[1;32m    403\u001b[0m         images\u001b[39m=\u001b[39;49mimages,\n\u001b[1;32m    404\u001b[0m         run_manager\u001b[39m=\u001b[39;49mrun_manager,\n\u001b[1;32m    405\u001b[0m         verbose\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mverbose,\n\u001b[1;32m    406\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m    407\u001b[0m     )\n\u001b[1;32m    408\u001b[0m     generations\u001b[39m.\u001b[39mappend([final_chunk])\n\u001b[1;32m    409\u001b[0m \u001b[39mreturn\u001b[39;00m LLMResult(generations\u001b[39m=\u001b[39mgenerations)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.1/lib/python3.11/site-packages/langchain_community/llms/ollama.py:309\u001b[0m, in \u001b[0;36m_OllamaCommon._stream_with_aggregation\u001b[0;34m(self, prompt, stop, run_manager, verbose, **kwargs)\u001b[0m\n\u001b[1;32m    300\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_stream_with_aggregation\u001b[39m(\n\u001b[1;32m    301\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    302\u001b[0m     prompt: \u001b[39mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    306\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any,\n\u001b[1;32m    307\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m GenerationChunk:\n\u001b[1;32m    308\u001b[0m     final_chunk: Optional[GenerationChunk] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m--> 309\u001b[0m     \u001b[39mfor\u001b[39;49;00m stream_resp \u001b[39min\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_create_generate_stream(prompt, stop, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs):\n\u001b[1;32m    310\u001b[0m         \u001b[39mif\u001b[39;49;00m stream_resp:\n\u001b[1;32m    311\u001b[0m             chunk \u001b[39m=\u001b[39;49m _stream_response_to_generation_chunk(stream_resp)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.1/lib/python3.11/site-packages/langchain_community/llms/ollama.py:154\u001b[0m, in \u001b[0;36m_OllamaCommon._create_generate_stream\u001b[0;34m(self, prompt, stop, images, **kwargs)\u001b[0m\n\u001b[1;32m    146\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_create_generate_stream\u001b[39m(\n\u001b[1;32m    147\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    148\u001b[0m     prompt: \u001b[39mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    151\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any,\n\u001b[1;32m    152\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Iterator[\u001b[39mstr\u001b[39m]:\n\u001b[1;32m    153\u001b[0m     payload \u001b[39m=\u001b[39m {\u001b[39m\"\u001b[39m\u001b[39mprompt\u001b[39m\u001b[39m\"\u001b[39m: prompt, \u001b[39m\"\u001b[39m\u001b[39mimages\u001b[39m\u001b[39m\"\u001b[39m: images}\n\u001b[0;32m--> 154\u001b[0m     \u001b[39myield from\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_create_stream(\n\u001b[1;32m    155\u001b[0m         payload\u001b[39m=\u001b[39;49mpayload,\n\u001b[1;32m    156\u001b[0m         stop\u001b[39m=\u001b[39;49mstop,\n\u001b[1;32m    157\u001b[0m         api_url\u001b[39m=\u001b[39;49m\u001b[39mf\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m{\u001b[39;49;00m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbase_url\u001b[39m}\u001b[39;49;00m\u001b[39m/api/generate/\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m    158\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m    159\u001b[0m     )\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.1/lib/python3.11/site-packages/langchain_community/llms/ollama.py:228\u001b[0m, in \u001b[0;36m_OllamaCommon._create_stream\u001b[0;34m(self, api_url, payload, stop, **kwargs)\u001b[0m\n\u001b[1;32m    226\u001b[0m \u001b[39mif\u001b[39;00m response\u001b[39m.\u001b[39mstatus_code \u001b[39m!=\u001b[39m \u001b[39m200\u001b[39m:\n\u001b[1;32m    227\u001b[0m     \u001b[39mif\u001b[39;00m response\u001b[39m.\u001b[39mstatus_code \u001b[39m==\u001b[39m \u001b[39m404\u001b[39m:\n\u001b[0;32m--> 228\u001b[0m         \u001b[39mraise\u001b[39;00m OllamaEndpointNotFoundError(\n\u001b[1;32m    229\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mOllama call failed with status code 404. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    230\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mMaybe your model is not found \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    231\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mand you should pull the model with `ollama pull \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\u001b[39m}\u001b[39;00m\u001b[39m`.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    232\u001b[0m         )\n\u001b[1;32m    233\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    234\u001b[0m         optional_detail \u001b[39m=\u001b[39m response\u001b[39m.\u001b[39mjson()\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39merror\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mOllamaEndpointNotFoundError\u001b[0m: Ollama call failed with status code 404. Maybe your model is not found and you should pull the model with `ollama pull llama2`."
     ]
    }
   ],
   "source": [
    "import importlib\n",
    "import custom_components.quiz_metric as qm\n",
    "importlib.reload(qm)\n",
    "from concordia.language_model import ollama_model\n",
    "importlib.reload(ollama_model)\n",
    "model = ollama_model.OllamaLanguageModel(\n",
    "    # model_name='llama2:70b',\n",
    "    model_name='mixtral'\n",
    "    # streaming=True\n",
    ")\n",
    "#@title Make the clock\n",
    "time_step = datetime.timedelta(minutes=20)\n",
    "SETUP_TIME = datetime.datetime(hour=20, year=1525, month=10, day=1)\n",
    "\n",
    "START_TIME = datetime.datetime(hour=18, year=1525, month=10, day=2)\n",
    "clock = game_clock.MultiIntervalClock(\n",
    "    start=SETUP_TIME,\n",
    "    step_sizes=[time_step, datetime.timedelta(seconds=10)])\n",
    "\n",
    "context = \"\"\"Elizabeth Harding, born in 1500 in the rural countryside of England, \n",
    "is a figure deeply entrenched in the tapestries of early Tudor England. Growing up in a \n",
    "modest farming family, Elizabeth's life has been shaped by the agrarian rhythms of sowing and \n",
    "harvesting. Her education, typical for a woman of her time and station, was minimal, focusing \n",
    "mainly on domestic skills essential for managing a household. She learned to spin wool, cook, \n",
    "and tend to livestock, but her literacy was limited to basic reading, a skill not commonly \n",
    "bestowed upon women of her era.Her life, however, was not confined to the domestic sphere alone. \n",
    "The early 16th century was a period of religious and political upheaval. The reign of Henry VIII \n",
    "saw dramatic changes, especially with the King's challenge to the Catholic Church, leading to the \n",
    "English Reformation. Elizabeth, like many of her contemporaries, found herself caught in the \n",
    "tides of these changes. Her family, staunchly Catholic, grappled with the shifting religious \n",
    "landscape, often discussing the latest decrees from the King that shook the foundations of \n",
    "their faith. In 1525, Elizabeth married Thomas Barton, a blacksmith, thereby entering a \n",
    "slightly higher social stratum. Their union, arranged by their families, was typical of the \n",
    "time, prioritizing social and economic considerations over romantic ones. Together, they \n",
    "navigated the complexities of Tudor society, experiencing both the challenges of rural life \n",
    "and the reverberations of decisions made in the distant courts of London. \n",
    "\n",
    "It is now 1525. The year 2000 has not occured yet. Elizabeth is taking a quiz on trivia questions, \n",
    "just for fun, to test her knowledge. If Elizabeth doesn't know the answer, she will stilll guess and \n",
    "begin her answer with a single choice. hHow would Elizabeth answer the following question?\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "quiz_metric = qm.QuizMetric(model, \"Elizabeth\", clock, \"./custom_components/trivia_questions.json\", measurements=measurements, verbose=True)\n",
    "quiz_metric.observe(context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "56adbe9f969e1c3cfd9bf2f14a39b20eccd0989e2dcbe7cf5bd90eb1142735b3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
