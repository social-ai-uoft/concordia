{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "J2TwJrZ08wXz"
      },
      "source": [
        "## Init and import"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This depends on a new custom module in concordia\n",
        "Run \"pip install --editable .[dev]\" in your environment to install the new build if you haven't"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "-qLG5ExLqpWa"
      },
      "outputs": [],
      "source": [
        "# @title Imports\n",
        "import sys\n",
        "import os\n",
        "path = os.path.abspath('..')\n",
        "if path not in sys.path:\n",
        "    sys.path.insert(0, path)\n",
        "import datetime\n",
        "\n",
        "from concordia import components as generic_components\n",
        "from concordia.agents import basic_agent\n",
        "from concordia.components import agent as components\n",
        "from concordia.agents import basic_agent\n",
        "from concordia.associative_memory import associative_memory\n",
        "from concordia.associative_memory import blank_memories\n",
        "from concordia.associative_memory import formative_memories\n",
        "from concordia.associative_memory import importance_function\n",
        "from concordia.clocks import game_clock\n",
        "from concordia.components import game_master as gm_components\n",
        "from concordia.environment import game_master\n",
        "from concordia.metrics import goal_achievement\n",
        "from concordia.metrics import common_sense_morality\n",
        "from concordia.metrics import opinion_of_others\n",
        "from concordia.utils import measurements as measurements_lib\n",
        "from concordia.utils import html as html_lib\n",
        "from concordia.utils import plotting\n",
        "\n",
        "import logging\n",
        "logging.basicConfig(level=logging.ERROR, filename='components_testing.log')\n",
        "logger = logging.getLogger('llama')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "I3OtW8flCJSC"
      },
      "outputs": [],
      "source": [
        "# Setup sentence encoder\n",
        "from sentence_transformers import SentenceTransformer\n",
        "st5_model = SentenceTransformer('sentence-transformers/sentence-t5-base')\n",
        "embedder = st5_model.encode"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {},
      "outputs": [],
      "source": [
        "# from concordia.language_model import huggingface_model\n",
        "import llama_cpp_model\n",
        "import importlib\n",
        "importlib.reload(llama_cpp_model)\n",
        "\n",
        "model = llama_cpp_model.LlamaCppLanguageModel(\n",
        "    repo_id=\"TheBloke/Llama-2-7B-Chat-GGUF\",\n",
        "    filename=\"llama-2-7b-chat.Q4_K_M.gguf\",\n",
        "    logits=True,\n",
        "    verbose=True,\n",
        ")\n",
        "\n",
        "# model = llama_cpp_model.LlamaCppLanguageModel(\n",
        "#     repo_id=\"TheBloke/Llama-2-70B-Chat-GGUF\",\n",
        "#     filename=\"llama-2-70b-chat.Q4_K_M.gguf\",\n",
        "#     logits=True,\n",
        "#     verbose=True,\n",
        "# )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [],
      "source": [
        "#@title Make the clock\n",
        "time_step = datetime.timedelta(minutes=20)\n",
        "SETUP_TIME = datetime.datetime(hour=20, year=2024, month=10, day=1)\n",
        "\n",
        "START_TIME = datetime.datetime(hour=18, year=2024, month=10, day=2)\n",
        "clock = game_clock.MultiIntervalClock(\n",
        "    start=SETUP_TIME,\n",
        "    step_sizes=[time_step, datetime.timedelta(seconds=10)])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "5RU3ZV4oIknW"
      },
      "outputs": [],
      "source": [
        "measurements = measurements_lib.Measurements()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The LLM response is: \n",
            "  Yes\n",
            "To the question \"1 + 1 = 2.\", the agent answered \"no\", the probs are {'yes': 0.2661084360524832, 'no': 0.7338915639475169}\n",
            "The LLM response is: \n",
            "  Yes\n",
            "To the question \"Red is a color.\", the agent answered \"no\", the probs are {'yes': 0.17441360838614256, 'no': 0.8255863916138575}\n",
            "The LLM response is: \n",
            "  No\n",
            "To the question \"Earth is flat.\", the agent answered \"no\", the probs are {'yes': 0.1716804534244546, 'no': 0.8283195465755454}\n",
            "The LLM response is: \n",
            "  Yes\n",
            "To the question \"There are 365 days in a year.\", the agent answered \"no\", the probs are {'yes': 0.131704773798717, 'no': 0.868295226201283}\n",
            "The LLM response is: \n",
            "  No\n",
            "To the question \"There aren't 365 days in a year.\", the agent answered \"no\", the probs are {'yes': 0.20670607283821446, 'no': 0.7932939271617856}\n",
            "The LLM response is: \n",
            "  Yes\n",
            "To the question \"The sum of the angles of a triangle is 180 degrees.\", the agent answered \"no\", the probs are {'yes': 0.19498616732805618, 'no': 0.8050138326719439}\n",
            "The LLM response is: \n",
            "  No\n",
            "To the question \"The printing press was invented in the 13th century.\", the agent answered \"no\", the probs are {'yes': 0.14345733860397364, 'no': 0.8565426613960263}\n",
            "The LLM response is: \n",
            "  Yes\n",
            "To the question \"The melting point of carbon is around 3500 degrees Celsius.\", the agent answered \"no\", the probs are {'yes': 0.15162970822174252, 'no': 0.8483702917782575}\n",
            "LogitQueryScores of Alice: Agent completed the quiz. 3 out of 8 questions are correct.\n"
          ]
        }
      ],
      "source": [
        "import custom_components.logits_query_metric as lm\n",
        "import importlib\n",
        "importlib.reload(lm)\n",
        "\n",
        "context = \"\"\"<<SYS>>\n",
        "You are a helpful, respectful and honest assistant.\n",
        "<</SYS>>\n",
        "\n",
        "If the following statement is true, answer \"yes\". If it is false, answer \"no\". Only answer with a single word.\n",
        "\"\"\"\n",
        "\n",
        "quiz_metric = lm.LogitsQueryMetric(model, \n",
        "                                   \"Alice\", \n",
        "                                   clock, \n",
        "                                   \"./custom_components/truth_or_lie.json\", \n",
        "                                   measurements=measurements, \n",
        "                                   verbose=True,\n",
        "                                   query=[\"yes\", \"no\"]) # query should match up with order of options in the question json\n",
        "quiz_metric.observe(context)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'quiz_metric' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[5], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mexamples\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcustom_components\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlogits_query_metric\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mlm\u001b[39;00m\n\u001b[1;32m      3\u001b[0m importlib\u001b[38;5;241m.\u001b[39mreload(lm)\n\u001b[0;32m----> 5\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mquiz_metric\u001b[49m\u001b[38;5;241m.\u001b[39mget_results()\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mcsv\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Create a csv file with the results if not already created\u001b[39;00m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'quiz_metric' is not defined"
          ]
        }
      ],
      "source": [
        "import importlib\n",
        "import examples.custom_components.logits_query_metric as lm\n",
        "importlib.reload(lm)\n",
        "\n",
        "results = quiz_metric.get_results()\n",
        "import csv\n",
        "# Create a csv file with the results if not already created\n",
        "if not os.path.exists('./results/'):\n",
        "    os.makedirs('./results/')\n",
        "with open('./results/logits.txt', 'w') as file:\n",
        "    # Write Question: Response, and space after each pair\n",
        "    writer = csv.writer(file)\n",
        "    for result in results:\n",
        "        writer.writerow(result)\n",
        "    file.close()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "last_runtime": {
        "build_target": "",
        "kind": "private"
      },
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
