{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "J2TwJrZ08wXz"
      },
      "source": [
        "## Init and import"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This depends on a new custom module in concordia\n",
        "Run \"pip install --editable .[dev]\" in your environment to install the new build if you haven't"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "-qLG5ExLqpWa"
      },
      "outputs": [],
      "source": [
        "# @title Imports\n",
        "import sys\n",
        "import os\n",
        "path = os.path.abspath('..')\n",
        "if path not in sys.path:\n",
        "    sys.path.insert(0, path)\n",
        "import datetime\n",
        "\n",
        "from concordia import components as generic_components\n",
        "from concordia.agents import basic_agent\n",
        "from concordia.components import agent as components\n",
        "from concordia.agents import basic_agent\n",
        "from concordia.associative_memory import associative_memory\n",
        "from concordia.associative_memory import blank_memories\n",
        "from concordia.associative_memory import formative_memories\n",
        "from concordia.associative_memory import importance_function\n",
        "from concordia.clocks import game_clock\n",
        "from concordia.components import game_master as gm_components\n",
        "from concordia.environment import game_master\n",
        "from concordia.metrics import goal_achievement\n",
        "from concordia.metrics import common_sense_morality\n",
        "from concordia.metrics import opinion_of_others\n",
        "from concordia.utils import measurements as measurements_lib\n",
        "from concordia.utils import html as html_lib\n",
        "from concordia.utils import plotting\n",
        "\n",
        "import logging\n",
        "logging.basicConfig(level=logging.ERROR, filename='components_testing.log')\n",
        "logger = logging.getLogger('huggingface')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "I3OtW8flCJSC"
      },
      "outputs": [],
      "source": [
        "# Setup sentence encoder\n",
        "from sentence_transformers import SentenceTransformer\n",
        "st5_model = SentenceTransformer('sentence-transformers/sentence-t5-base')\n",
        "embedder = st5_model.encode"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cuda:0\n"
          ]
        }
      ],
      "source": [
        "# from concordia.language_model import huggingface_model\n",
        "import huggingface_model\n",
        "import importlib\n",
        "importlib.reload(huggingface_model)\n",
        "# ===== MODEL LIST =====\n",
        "# bigscience/bloom-560m\n",
        "# mistralai/Mixtral-8x7B-v0.1 (not tested yet)\n",
        "# mistralai/Mistral-7B-v0.1\n",
        "# (check huggingface for more)\n",
        "\n",
        "model = huggingface_model.HuggingFaceLanguageModel(\n",
        "    model_name='bigscience/bloom-560m',\n",
        "    logits=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "#@title Make the clock\n",
        "time_step = datetime.timedelta(minutes=20)\n",
        "SETUP_TIME = datetime.datetime(hour=20, year=2024, month=10, day=1)\n",
        "\n",
        "START_TIME = datetime.datetime(hour=18, year=2024, month=10, day=2)\n",
        "clock = game_clock.MultiIntervalClock(\n",
        "    start=SETUP_TIME,\n",
        "    step_sizes=[time_step, datetime.timedelta(seconds=10)])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "5RU3ZV4oIknW"
      },
      "outputs": [],
      "source": [
        "measurements = measurements_lib.Measurements()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "To the question 1 + 1 = 2, the agent answered \"no\", the probs are {'yes': 0.2863, 'no': 0.7137}\n",
            "To the question Red is a color, the agent answered \"no\", the probs are {'yes': 0.1309, 'no': 0.8691}\n",
            "To the question Earth is flat, the agent answered \"no\", the probs are {'yes': 0.001, 'no': 0.999}\n",
            "To the question There are 365 days in a year, the agent answered \"no\", the probs are {'yes': 0.01, 'no': 0.99}\n",
            "To the question There aren't 365 days in a year, the agent answered \"no\", the probs are {'yes': 0.0104, 'no': 0.9896}\n",
            "To the question The sum of the angles of a triangle is 180 degrees, the agent answered \"no\", the probs are {'yes': 0.0094, 'no': 0.9906}\n",
            "To the question The printing press was invented in the 13th century, the agent answered \"no\", the probs are {'yes': 0.138, 'no': 0.862}\n",
            "To the question The melting point of carbon is around 3500 degrees Celsius, the agent answered \"no\", the probs are {'yes': 0.0398, 'no': 0.9602}\n",
            "LogitQueryScores of Alice: Agent completed the quiz. 3 out of 8 questions are correct.\n"
          ]
        }
      ],
      "source": [
        "import custom_components.logits_query_metric as lm\n",
        "import importlib\n",
        "importlib.reload(lm)\n",
        "\n",
        "context = \"\"\"\n",
        "Below is a statement. If it is true, answer \"yes\". If it is false, answer \"no\".\n",
        "\"\"\"\n",
        "\n",
        "quiz_metric = lm.LogitsQueryMetric(model, \n",
        "                                   \"Alice\", \n",
        "                                   clock, \n",
        "                                   \"./custom_components/truth_or_lie.json\", \n",
        "                                   measurements=measurements, \n",
        "                                   verbose=True,\n",
        "                                   query=[\"yes\", \"no\"]) # query should match up with order of options in the question json\n",
        "quiz_metric.observe(context)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "import importlib\n",
        "import examples.custom_components.logits_query_metric as lm\n",
        "importlib.reload(lm)\n",
        "\n",
        "results = quiz_metric.get_results()\n",
        "import csv\n",
        "# Create a csv file with the results if not already created\n",
        "if not os.path.exists('./results/'):\n",
        "    os.makedirs('./results/')\n",
        "with open('./results/logits.txt', 'w') as file:\n",
        "    # Write Question: Response, and space after each pair\n",
        "    writer = csv.writer(file)\n",
        "    for result in results:\n",
        "        writer.writerow(result)\n",
        "    file.close()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "last_runtime": {
        "build_target": "",
        "kind": "private"
      },
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
